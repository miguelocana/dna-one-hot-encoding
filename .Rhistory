# Transforma los promotores en factores + (1) y - (0)
for (i in 1:length(vector)) {
if (vector[i]==1) {
vector[i] <- '+'
} else {
vector[i] <- '-'
}
}
vector <- as.factor(vector)
return (vector)
}
##################################### ALGORITMOS ######################################
kneighbors <- function(df_onehot) {
# Split train y data
train <- split_train_test(df_onehot)$train[,-1]
test <- split_train_test(df_onehot)$test[,-1]
train_labels <- as.factor(split_train_test(df_onehot)$train[,1])
test_labels <- as.factor(split_train_test(df_onehot)$test[,1])
# kNN 1
# Modelo y estad?sticos
knn1 <- knn(train = train, test = test, cl = train_labels, k = 1)
# Predicciones
kneighbors_1_p <- confusionMatrix(knn1, test_labels)
# kNN 3
# Modelo y estad?sticos
knn3 <- knn(train = train, test = test, cl = train_labels, k = 3)
# Predicciones
kneighbors_3_p <- confusionMatrix(knn3, test_labels)
# kNN 5
# Modelo y estad?sticos
knn5 <- knn(train = train, test = test, cl = train_labels, k = 5)
# Predicciones
kneighbors_5_p <- confusionMatrix(knn5, test_labels)
# kNN 7
# Modelo y estad?sticos
knn7 <- knn(train = train, test = test, cl = train_labels, k = 7)
# Predicciones
#kneighbors_7_p <- CrossTable(x = test_labels, y = knn7, prop.chisq=FALSE)
kneighbors_7_p <- confusionMatrix(knn7, test_labels)
return (list('k1'=kneighbors_1_p,'k3'=kneighbors_3_p,'k5'=kneighbors_5_p,'k7'=kneighbors_7_p))
}
naive_bayes <- function (df) {
# - Se explorar? la opci?n de activar o no 'laplace'.
# - Tanto las variables dependientes como independientes deben ser de tipo factor.
# - Se separan los clasificadores de train y test.
# Transforma todas las columnas en factores
for (i in 1:ncol(df)) {
# Comprueba si es factor, de lo contrario lo transforma
if (is.factor(df[,i])) {
next
} else {
df[,i] <- as.factor(df[,i])
}
}
# Split train y data
train <- split_train_test(df)$train[,-1]
test <- split_train_test(df)$test[,-1]
train_labels <- as.factor(split_train_test(df)$train[,1])
test_labels <- as.factor(split_train_test(df)$test[,1])
# Laplace = 0
# Modelo y estad?sticos
NBlp0 <- e1071::naiveBayes(train, train_labels, laplace = 0)
# Predicciones
pNBlp0 <- predict(NBlp0, test)
c1 <- confusionMatrix(pNBlp0, test_labels, dnn = c('Predicho','Actual'))
# Interpretaci?n final:
# - El modelo tiene una precisi?n del 90%
# - Devuelve 3 falsos negativos
# Laplace = 1
# Modelo y estad?sticos
NBlp1 <- e1071::naiveBayes(train, train_labels, laplace = 1)
# Predicciones
pNBlp1 <- predict(NBlp1, test)
c2 <- confusionMatrix(pNBlp1, test_labels, dnn = c('Predicho','Actual'))
# Interpretaci?n final:
# - El modelo tiene una precisi?n del 84%
# - Devuelve 2 falsos positivos
# - Devuelve 1 falso negativo
return (list('NBlp0'=c1, 'NBlp1'=c2))
}
decision_tree <- function(df_onehot) {
# - Hay que separar la variable dependiente de los predictores en train y test
# - El vector con la variable dependiente debe ser un factor (al menos con C5.0)
# Split train y data
train <- split_train_test(df_onehot)$train[,-1]
test <- split_train_test(df_onehot)$test[,-1]
train_labels <- as.factor(split_train_test(df_onehot)$train[,1])
test_labels <- as.factor(split_train_test(df_onehot)$test[,1])
# Sin boosting
# Modelo y estad?sticos
DTsb <- C5.0(train, train_labels)
# Predicciones
pDTsb <- predict(DTsb, test)
c1 <- confusionMatrix(pDTsb, test_labels, dnn = c('Predicho','Actual'))
# Con boosting
# Modelo y estad?sticos
DTnb <- C5.0(train, train_labels, trials = 10)
# Predicciones
pDTnb <- predict(DTnb, test)
c2 <- confusionMatrix(pDTnb, test_labels, dnn = c('Predicho','Actual'))
return (list('DTsb'=c1, 'DTnb'=c2))
}
random_forest <- function(df_onehot) {
# - Se explorar?n la opci?n de n?mero de ?rboles n - 50, 100
# - Hay que separar la variable dependiente de los predictores en train y test
# - La variable dependiente debe ser un factor
# Convierte la clase en factor
df_onehot[,1] <- as.factor(df_onehot[,1])
# Split train y data y labels
train <- split_train_test(df_onehot)$train[,-1]
test <- split_train_test(df_onehot)$test[,-1]
train_labels <- split_train_test(df_onehot)$train[,1]
test_labels <- split_train_test(df_onehot)$test[,1]
# RF 50
# Modelo y estad?sticos
RF50 <- randomForest(train, train_labels, ntree = 50)
# Predicciones
pRF50 <- predict(RF50, test)
confusionMatrix(pRF50, test_labels, dnn = c('Predicho','Actual'))
# Interpretaci?n final:
# - El modelo tiene una precisi?n del 93%
# - Devuelve 1 falso positivo
# - Devuelve 1 falso negativo
# RF 100
# Modelo y estad?sticos
RF100 <- randomForest(train, train_labels, ntree = 100)
# Predicciones
pRF100 <- predict(RF100, test)
c2 <- confusionMatrix(pRF100, test_labels, dnn = c('Predicho','Actual'))
# Interpretaci?n final:
# - El modelo tiene una precisi?n del 87%
# - Devuelve 2 falsos positivos
# - Devuelve 2 falsos negativos
return (list('RF50'=c1, 'RF100'=c2))
}
svm <- function(df_onehot, formula) {
# - Se explorar?n las funciones kernel lineal y rbf.
# - Las variables independientes deben ser num?ricas y estar normalizadas.
# - Las variables dependientes deben ser de tipo factor.
# - Se separan los clasificadores de train y test.
# Transforma la clase en tipo factor
df_onehot[,1] <- as.factor(df_onehot[,1])
# Split train y data
train <- split_train_test(df_onehot)$train
test <- split_train_test(df_onehot)$test
# Kernel lineal
# Modelo y estad?sticos
SVM_lineal <- ksvm(formula, data = train, kernel = 'vanilladot')
# Predicciones
pSVM_lineal <- predict(SVM_lineal, test)
c1 <- confusionMatrix(pSVM_lineal, test[,1], dnn = c('Predicho','Actual'))
# Intepretaci?n:
# - El modelo tiene una precisi?n del 90%
# - Devuelve 2 falsos positivos
# - Devuelve 1 falsos negativos
# RBF
# Modelo y estad?sticos
SVM_rbf <- ksvm(formula, data = train, kernel = 'rbfdot')
# Predicciones
pSVM_rbf <- predict(SVM_rbf, test)
c2 <- confusionMatrix(pSVM_rbf, test[,1], dnn = c('Predicho','Actual'))
# Intepretaci?n:
# - El modelo tiene una precisi?n del 90%
# - Devuelve 1 falso positivo
# - Devuelve 2 falsos negativos
return (list('SVM_lineal'=c1, 'SVM_rbf'=c2))
}
ann <- function(df_onehot, formula) {
# - Se explorarán el número de nodos de la capa oculta n = 4,5
# - Las variables independientes y dependientes deben ser numéricas (entre 0 y 1)
# Se transforma la variable dependiente en tipo numérico
df_onehot[,1] <- cat_to_num(df_onehot[,1])
# Split train y data
train <- split_train_test(df_onehot)$train
test <- split_train_test(df_onehot)$test
# ANN 4
# Modelo y estad?sticos
ANN4 <- neuralnet(formula, data = train, hidden = 4,linear.output = FALSE )
# Predicciones
pANN4 <- round(compute(ANN4,test)$net.result,digits=0)
# Se transforma en factores '+' o '-' y se evalúa el modelo
test4 <- test
test4[,1] <- num_to_cat(test4[,1])
pANN4 <- num_to_cat(pANN4)
c1 <- confusionMatrix(pANN4, test4[,1], dnn = c('Predicho','Actual'))
# Intepretaci?n:
# - El modelo tiene una precisión del 90%
# - Devuelve 2 falsos positivos
# - Devuelve 1 falso negativo
# ANN 5
# Modelo y estad?sticos
ANN5 <- neuralnet(formula, data = train, hidden = 5,linear.output = FALSE )
# Predicciones
pANN5 <- round(compute(ANN5,test)$net.result,digits=0)
# Se transforma en factores '+' o '-' y se evalúa el modelo
test5 <- test
test5[,1] <- num_to_cat(test5[,1])
pANN5 <- num_to_cat(pANN5)
c2 <- confusionMatrix(pANN5, test5[,1], dnn = c('Predicho','Actual'))
# Intepretaci?n:
# - El modelo tiene una precisión del 90%
# - Devuelve 2 falsos positivos
# - Devuelve 1 falso negativo
return (list('ANN4'=c1, 'ANN5'=c2))
}
##################################### DATAFRAMES #######################################
df_onehot <- onehot(df = DF[,-2], col_seq = 'V3')
df_categorico <- sepseq(df = DF[,-2], col_seq = 'V3')
NB <- naive_bayes(df_categorico)
KNN <- kneighbors(df_onehot)
ANN <- ann(df_onehot, formula)
SVM <- svm(df_onehot, formula)
RF <- random_forest(df_onehot)
DT <- decision_tree(df_onehot)
NB <- naive_bayes(df_categorico)
KNN <- kneighbors(df_onehot)
ANN <- ann(df_onehot, FORMULA)
SVM <- svm(df_onehot, FORMULA)
RF <- random_forest(df_onehot)
DT <- decision_tree(df_onehot)
data.frame(c('+','-','+','-','-','+'))
data.frame(c('+','-','+','-','-','+'),c('agtt','ACTG','GCTA','actg','gtca','ccct'))
data.frame(promoter=c('+','-','+','-','-','+'),seq=c('agtt','ACTG','GCTA','actg','gtca','ccct'),)
data.frame(promoter=c('+','-','+','-','-','+'),seq=c('agtt','ACTG','GCTA','actg','gtca','ccct'))
promoters <- read.csv('promoters.txt',header = FALSE,sep=',')
promoters <- read.csv('promoters.txt',header = FALSE,sep=',')
str(promoters)
promoters2 <- promoters[,-2]
promoters2
promoters2 <- promoters[,-2]
NB <- naive_bayes(df_categorico)
knn1 <- knn(train = train, test = test, cl = train_labels, k = 1)
KNN$k1
KNN$k1
KNN$k3
KNN$k5
KNN$k7
NBlp0 <- e1071::naiveBayes(train, train_labels, laplace = 0)
NBlp0 <- e1071::naiveBayes(train, train_labels, laplace = 0)
NB <- naive_bayes(df_categorico)
KNN <- kneighbors(df_onehot)
ANN <- ann(df_onehot, FORMULA)
SVM <- svm(df_onehot, FORMULA)
RF <- random_forest(df_onehot)
DT <- decision_tree(df_onehot)
NB$NBlp0[0]
NB$NBlp0[1]
NB$NBlp0[2]
NB$NBlp0[3]
NB$NBlp0[3]$overall
NB$NBlp0[3][1]
NB$NBlp0[3][3]
NB$NBlp0[3][2]
NB$NBlp0[3][1]
NB$NBlp0$overall.accuracy
NB$NBlp0$overall['Accuracy']
x <- data.frame(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy'])
View(x)
x <- data.frame(accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))
View(x)
x <- data.frame(
modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),
feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),
accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))
View(x)
plot(x$accuracy)
order(x)
x[order(x)]
x[order(x$accuracy)]
x[order(x$accuracy),]
x[order(x$accuracy,decreasing = TRUE),]
# 3. Discusión final
```{r echo=FALSE}
hist(x$accuracy)
hist(x)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))[order(x$accuracy,decreasing = TRUE),c()]
x
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))[order(x$accuracy,decreasing = TRUE)]
x[order(x$accuracy,decreasing = TRUE),c()]
x[order(x$accuracy,decreasing = TRUE),]
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))
x[order(x$accuracy,decreasing = TRUE),]
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']))[order(x$accuracy,decreasing = TRUE),]
x
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan")
hist(x$accuracy,
col=colors)
barplot(x)
barplot(x$accuracy)
barplot(x$accuracy, main = "Accuracy")
barplot(x$accuracy, ylab = "accuracy")
barplot(x$accuracy, main = 'Resultados modelos', ylab = "accuracy")
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy")
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo, beside=TRUE)
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan")
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo,
col = c("lightblue",
"lightcyan", "lavender", "mistyrose",  "cornsilk")
)
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan", "red")
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo,
col = colors
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo,
col = colors,
horiz = TRUE
)
x2 <- x[order(x$accuracy,decreasing = TRUE),]
x2
barplot(x2$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo,
col = colors,
horiz = TRUE,
)
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan", "red")
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend = x$modelo,
col = colors,
horiz = TRUE,
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy", legend.text = x$modelo,
col = colors,
horiz = TRUE,
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = colors,
horiz = TRUE,
)
text(0.5*x[1,-1])
text(0.5*x[1,-1], x$accuracy, colnames(x$modelo))
text(, x$accuracy, colnames(x$modelo))
text(x$accuracy, x$accuracy, colnames(x$modelo))
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = colors,
horiz = TRUE,
)
text(x$accuracy, x$accuracy, colnames(x$modelo))
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = colors,
horiz = TRUE,
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = colors,
horiz = TRUE,
names = x$modelo
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = "#69b3a2",
horiz = TRUE,
names = x$modelo
)
barplot(x$accuracy, main = 'Resultado modelos', ylab = "accuracy",
col = "#69b3a2",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .5
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "#69b3a2",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .5
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "#69b3a2",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .7
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "#69b3a2",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
c('red','red','yellow','yellow','yellow','yellow','green','green','violet','violet','orange','orange'))
barplot(x$accuracy, main = 'Resultado modelos',
col = "lightblue",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "lightred",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "lightorange",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "lightblue",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "darkolivegreen2",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy, main = 'Resultado modelos',
col = "coral",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
c('coral','coral','yellow','yellow','yellow','yellow','darkolivegreen2','darkolivegreen2','blueviolet','blueviolet','orange','orange','ligthblue','ligthblue'))
barplot(x$accuracy, main = 'Resultado modelos',
col = "coral",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x[order(x$accuracy,decreasing = TRUE),]
barplot(x$accuracy, main = 'Resultado modelos',
col = "coral",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- x[order(x$accuracy,decreasing = TRUE),]
barplot(x$accuracy, main = 'Resultado modelos',
col = "coral",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- x[order(x$accuracy,decreasing = FALSE),]
barplot(x$accuracy, main = 'Resultado modelos',
col = "coral",
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
colors = c('coral','coral','yellow','yellow','yellow','yellow','darkolivegreen2','darkolivegreen2','blueviolet','blueviolet','orange','orange','ligthblue','ligthblue'))
x <- x[order(x$accuracy,decreasing = FALSE),]
barplot(x$accuracy, main = 'Resultado modelos',
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
colors = c('coral','coral','yellow','yellow','yellow','yellow','darkolivegreen2','darkolivegreen2','blueviolet','blueviolet','orange','orange','lightblue','lightblue'))
x <- x[order(x$accuracy,decreasing = FALSE),]
barplot(x$accuracy, main = 'Resultado modelos',
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
colors = c('red','red','yellow','yellow','yellow','yellow','darkolivegreen2','darkolivegreen2','blueviolet','blueviolet','orange','orange','cyan','cyan'))
x <- x[order(x$accuracy,decreasing = FALSE),]
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan", "red")
barplot(x$accuracy, main = 'Resultado modelos',
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
x <- data.frame(modelo = c('Naive Bayes', 'Naive Bayes', 'kNN', 'kNN', 'kNN', 'kNN', 'ANN', 'ANN', 'SVM', 'SVM', 'Random Forest', 'Random Forest', 'Decision Tree', 'Decision Tree'),feature = c('laplace = 0', 'laplace = 1', 'k = 1', 'k = 3', 'k = 5', 'k = 7', 'h = 4', 'h = 5', 'lineal', 'rbf', 'n = 50', 'n = 100', 'sin boosting', 'con boosting'),accuracy = c(NB$NBlp0$overall['Accuracy'], NB$NBlp1$overall['Accuracy'], KNN$k1$overall['Accuracy'], KNN$k3$overall['Accuracy'], KNN$k5$overall['Accuracy'], KNN$k7$overall['Accuracy'], ANN$ANN4$overall['Accuracy'], ANN$ANN5$overall['Accuracy'], SVM$SVM_lineal$overall['Accuracy'], SVM$SVM_rbf$overall['Accuracy'], RF$RF50$overall['Accuracy'], RF$RF100$overall['Accuracy'], DT$DTsb$overall['Accuracy'], DT$DTnb$overall['Accuracy']),
colors = c('red','red','yellow','yellow','yellow','yellow','green','green','blueviolet','blueviolet','orange','orange','cyan','cyan'))
x <- x[order(x$accuracy,decreasing = FALSE),]
colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan", "red")
barplot(x$accuracy, main = 'Resultado modelos',
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .9
)
barplot(x$accuracy,
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .6
)
barplot(x$accuracy,
col = x$colors,
horiz = TRUE,
names = x$modelo,
las = 2,
cex.names = .5
)
